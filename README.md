# Red-code-annotation-using-Spark
本项目为作者大二下学期《大数据计算基础》课程的大作业，该项目由作者团队历时两周完成，内容为利用Spark大数据平台实现对与新冠感染人员同时间、在同一个基站的人员进行精确红码标注，正式数据集解压后大小为22.9G。[项目具体要求](大作业要求.docx)

作者队伍最终成绩：总用时23min（下载与解压文件用时17min，运行代码用时6min）
最终成绩并不理想，因为使用校园网在百度网盘下载数据网速并不稳定

作者在网速正常时用大小相同的测试数据进行全过程最快速度为13min（下载与解压文件用时7min，运行代码用时6min）
## 思路与方案选择
作者与组内同学经过短时间的讨论，确定几种可行方案如下：
1. 使用Linux虚拟机进行Spark完全分布式集群的搭建
2. Windows系统下的Spark local模式的搭建
3. 利用docker完成Windows系统下Spark环境的搭建

作者确定这几种方案的原因如下：
1. 在本学期之前课程的学习过程中已经与组内同学完成了Hadoop完全分布式集群的搭建，集群结构为一主四从。而Spark完全分布式的搭建只需在此基础上完成对Spark开发环境的搭建即可
2. 虽然方案一已经可以完成本次项目要求，但由于本次项目的竞速性质，作者考虑到虚拟机可能无法调用电脑的全部性能，导致处理速度较慢
3. 同时考虑到在Windows系统搭建Spark开发环境可能涉及较多环境变量配置的操作，作者也考虑到是否可以利用docker容器进行快速部署

## 实践流程
1. 作者与组内同学首先成功完成了方案一中Linux系统下Spark集群的搭建，但在进一步尝试完成项目要求时遇到了诸多问题：
   
   1. 虚拟机存储空间不足
   2. Spark完全分布式集群若想在本地运行，需要将数据储存在所有节点的相同路径下，而且本地运行不太稳定。若将数据先上传至HDFS，由于文件较大，且校园网不太稳定，耗时预估两个小时左右。

2. 此时组内另一位同学完成了Windows系统下Spark集群的搭建，并完成了Scala代码的编写与运行

   但这种方案因为需要打包程序，提交到终端运行，考虑到还有优化空间，所以我们进一步考虑完成pyspark的方案

3. pyspark

   作者进一步实现了在pycharm上pyspark环境的配置，并完成了pyspark代码的编写
   
   期间因为作者第一次接触pyspark库，debug的过程异常艰难

   所以作者小组内同学自己编写了一个数据集，数据量只有50行左右，但其中包含了符合项目条件的所有正常与特殊情况[cdinfo.txt](DATA/cdinfo_s.txt), [infexted.txt](DATA/infected_s.txt)

   [数据集说明](DATA/感染者（类型）.txt)
